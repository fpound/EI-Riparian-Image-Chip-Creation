{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f3b133-2260-4ed2-b899-3d8fbaceec97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pysheds.grid import Grid\n",
    "\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.fill import fillnodata\n",
    "from rasterio.mask import mask\n",
    "from rasterio.enums import ColorInterp\n",
    "\n",
    "\n",
    "from shapely.geometry import mapping, shape, Polygon\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import ee\n",
    "## needs one time only initialization in terminal -> ?\n",
    "\n",
    "\n",
    "#Simple edit\n",
    "#import geemap\n",
    "\n",
    "\n",
    "import shutil\n",
    "\n",
    "from retry import retry\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a324ff82-b660-44a2-8809-753c3b484258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f62ef-ae98-4915-a8b7-f63e8aeb11cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb200a4b-6f32-4876-abda-24ed60f7267d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def makeGDF(shape_path):\n",
    "    shape_gdf = gpd.read_file(shape_path)\n",
    "    return shape_gdf\n",
    "\n",
    "\n",
    "\n",
    "def makeExtentPoly(raster_path):\n",
    "    \n",
    "    raster = rasterio.open(raster_path)\n",
    "    raster_name = os.path.split(InputData)[-1].split('.')[0]\n",
    "    \n",
    "    \n",
    "    left = raster.bounds[0] \n",
    "    lower = raster.bounds[1]\n",
    "    right = raster.bounds[2]\n",
    "    upper = raster.bounds[3]\n",
    "\n",
    "    # Set raster bounding box\n",
    "    raster_box = [[left, upper], [right, upper], [right, lower], [left, lower]]\n",
    "\n",
    "    # define the geometry\n",
    "    raster_poly = Polygon(raster_box)\n",
    "    d = {'name':[f'{raster_name}',], 'geometry': [raster_poly,]}\n",
    "\n",
    "    # make the geodataframe\n",
    "    raster_extent = gpd.GeoDataFrame(d, crs=str(raster.crs))\n",
    "    \n",
    "    return raster_extent\n",
    "\n",
    "\n",
    "\n",
    "def makeExtent_fromSHP(shp_path):\n",
    "\n",
    "    shp = gpd.read_file(shp_path)\n",
    "    shp_name = os.path.split(InputData)[-1].split('.')[0]\n",
    "    \n",
    "    \n",
    "    left = shp.total_bounds[0] \n",
    "    lower = shp.total_bounds[1]\n",
    "    right = shp.total_bounds[2]\n",
    "    upper = shp.total_bounds[3]\n",
    "\n",
    "    # Set raster bounding box\n",
    "    shp_box = [[left, upper], [right, upper], [right, lower], [left, lower]]\n",
    "\n",
    "    # define the geometry\n",
    "    shp_poly = Polygon(shp_box)\n",
    "    d = {'name':[f'{shp_name}',], 'geometry': [shp_poly,]}\n",
    "\n",
    "    # make the geodataframe\n",
    "    shp_extent = gpd.GeoDataFrame(d, crs=str(shp.crs))\n",
    "    \n",
    "    return shp_extent\n",
    "\n",
    "\n",
    "\n",
    "def makeExtent(InputData):\n",
    "\n",
    "    if InputData.endswith('.shp'):\n",
    "        extent = makeGDF(InputData)\n",
    "\n",
    "    elif InputData.endswith('.tif'):\n",
    "        extent = makeExtentPoly(InputData)\n",
    "\n",
    "    else:\n",
    "        print(\"Data type not known. Must be a .shp or .tif\")\n",
    "    return extent\n",
    "\n",
    "\n",
    "\n",
    "def Download_DEM(path):\n",
    "    \n",
    "    shape_gdf = makeExtent(path)\n",
    "\n",
    "    xMin = round(shape_gdf.total_bounds[0],2)\n",
    "    yMin = round(shape_gdf.total_bounds[1], 2)\n",
    "    xMax = round(shape_gdf.total_bounds[2], 2)\n",
    "    yMax = round(shape_gdf.total_bounds[3], 2)\n",
    "    boundingBox = \"{},{},{},{}\".format(xMin, yMin, xMax, yMax)\n",
    "\n",
    "\n",
    "    linkStart = \"https://tnmaccess.nationalmap.gov/api/v1/products?datasets=National%20Elevation%20Dataset%20(NED)%201/3%20arc-second&\"\n",
    "    boundingBox_forLink = \"bbox=\" + str(boundingBox)\n",
    "    linkFinish = \"&prodFormats=GeoTIFF&outputFormat=JSON\"\n",
    "    TNM_Link = linkStart + boundingBox_forLink + linkFinish\n",
    "\n",
    "    r = requests.get(TNM_Link)\n",
    "    json_data = r.json()\n",
    "    downloadList = []\n",
    "\n",
    "    for item in json_data[\"items\"]:\n",
    "        downloadList.append(item[\"downloadURL\"])\n",
    "\n",
    "    fileCounter = 1\n",
    "    RastersList = []\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    newFolder = os.path.split(path)[-1].split('.')[0]\n",
    "\n",
    "    x = os.path.join('DEM', newFolder)\n",
    "    if not os.path.isdir(x):\n",
    "        os.mkdir(x)\n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "    for downURL in downloadList:\n",
    "        fileSplit = downURL.split(\"/\")\n",
    "        fileName = fileSplit[-1]\n",
    "\n",
    "        \n",
    "        filePath = os.path.join('DEM', newFolder, fileName)\n",
    "        RastersList.append(filePath)\n",
    "\n",
    "        print (f'Downloading {fileName} file {fileCounter} of {len(downloadList)}')\n",
    "\n",
    "        fileDown = requests.get(downURL)\n",
    "\n",
    "        with open(filePath, 'wb') as asdf:\n",
    "            asdf.write(fileDown.content)\n",
    "        fileCounter += 1\n",
    "        \n",
    "    \n",
    "    return RastersList\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def watershed_preProcessing(dem_path):\n",
    "\n",
    "    DEM_folder  = os.path.splitext(dem_path)[0].split('/')[0]\n",
    "    #img_name = os.path.splitext(dem_path)[0].split('/')[1]\n",
    "    #fdir_dir = f'{DEM_folder}/{img_name}_fdir'\n",
    " \n",
    "    #if not os.path.isdir(fdir_dir):\n",
    "    #    os.mkdir(fdir_dir)\n",
    "        \n",
    "    \n",
    "    DEM_name = dem_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "    \n",
    "    grid = Grid.from_raster(dem_path, data_name='dem')\n",
    "    dem = grid.read_raster(dem_path)\n",
    "\n",
    "    # Detect pits\n",
    "    pits = grid.detect_pits(dem)\n",
    "\n",
    "    # Fill pits\n",
    "    pit_filled_dem = grid.fill_pits(dem)\n",
    "    pits = grid.detect_pits(pit_filled_dem)\n",
    "    assert not pits.any()\n",
    "\n",
    "    # Detect depressions\n",
    "    depressions = grid.detect_depressions(pit_filled_dem)\n",
    "\n",
    "    # Fill depressions\n",
    "    flooded_dem = grid.fill_depressions(pit_filled_dem)\n",
    "    depressions = grid.detect_depressions(flooded_dem)\n",
    "    assert not depressions.any()\n",
    "\n",
    "    # Detect flats\n",
    "    flats = grid.detect_flats(flooded_dem)\n",
    "\n",
    "    # Fill flats\n",
    "    inflated_dem = grid.resolve_flats(flooded_dem)\n",
    "    flats = grid.detect_flats(inflated_dem)\n",
    "\n",
    "    # Compute flow direction based on corrected DEM\n",
    "    fdir = grid.flowdir(inflated_dem)\n",
    "    \n",
    "#    fdir_path = 'asdf_fdir.tiff'\n",
    "    \n",
    "#    grid.to_raster(fdir, fdir_path, dtype=np.uint8)\n",
    "    \n",
    "    acc = grid.accumulation(fdir)\n",
    "    branches = grid.extract_river_network(fdir, acc > 2500)\n",
    "    \n",
    "    geojson_path = f'StreamVectors/{DEM_name}_sv.geojson'\n",
    "    f = open(geojson_path, 'w')\n",
    "    f.write(str(branches))\n",
    "    f.close()\n",
    "    \n",
    "    streamNet = gpd.read_file(geojson_path)\n",
    "    shp_path = f'StreamVectors/{DEM_name}_sv.shp'\n",
    "    streamNet.to_file(shp_path)\n",
    "    \n",
    "    #os.remove(geojson_path)\n",
    "    \n",
    "    return shp_path\n",
    "\n",
    "\n",
    "def makePointsFromLines(Line_SHP, Distance_Points_M):\n",
    "    \n",
    "\n",
    "    shp_name = os.path.split(Line_SHP)[-1].split('.')[0]\n",
    "\n",
    "    \n",
    "    SHP = gpd.read_file(Line_SHP)\n",
    "    \n",
    "    ## need to reproject for to the points to work correctly\n",
    "    ## need to come up with a way to identify the epsg programatically\n",
    "    \n",
    "    rePro_shp = SHP.to_crs(\"EPSG:3857\")\n",
    "    rePro_shp_path = f'{shp_name}_repro.shp'\n",
    "    rePro_shp.to_file(rePro_shp_path)\n",
    "    \n",
    "    \n",
    "    lines = fiona.open(rePro_shp_path)\n",
    "\n",
    "    # creation of the resulting shapefile\n",
    "    schema = {'geometry': 'Point','properties': {'id': 'int'}}\n",
    "\n",
    "    crs = lines.crs\n",
    "    points_path = f'POINTS/{DEM_name}_Points.shp'\n",
    "    points_repro_path = f'POINTS/{DEM_name}_Points_repro.shp'\n",
    "    \n",
    "    \n",
    "    with fiona.open(points_path,\n",
    "                    'w', 'ESRI Shapefile', schema, crs=crs) as output:\n",
    "\n",
    "        for line in lines:\n",
    "\n",
    "            geom = shape(line['geometry'])\n",
    "\n",
    "            # length of the LineString\n",
    "            length = geom.length\n",
    "\n",
    "            # create points every 10 meters along the line\n",
    "            for i, distance in enumerate(range(0, int(length), Distance_Points_M)):\n",
    "                point = geom.interpolate(distance)   \n",
    "                output.write({'geometry':mapping(point),'properties': {'id':i}}) \n",
    "    \n",
    "    \n",
    "        \n",
    "    for filename2 in glob.glob(f\"{rePro_shp_path[:-4]}*\"):\n",
    "        os.remove(filename2)\n",
    "    \n",
    "    points_gdf = gpd.read_file(points_path)\n",
    "    points_repro = points_gdf.to_crs(DEM.crs)\n",
    "    points_repro['id'] = np.arange(len(points_repro))\n",
    "\n",
    "    \n",
    "    return points_repro, points_path\n",
    "\n",
    "\n",
    "\n",
    "def points_toNAIP_Chips(points, inputName, optString=\"\"):\n",
    "    if points.empty:\n",
    "        return\n",
    "    #points.to_crs(DEM.crs)\n",
    "    \n",
    "    points_gee = []\n",
    "    for i in range(0, len(points)):\n",
    "        \n",
    "        x = float(points.iloc[i].geometry.centroid.x)\n",
    "        y = float(points.iloc[i].geometry.centroid.y)\n",
    "        geom = {'geodesic': False,\n",
    "                'type': 'Point', \n",
    "                'coordinates': [x, y]}\n",
    "        points_gee.append(geom)\n",
    "    #points_gee\n",
    "\n",
    "    left = points.total_bounds[0] \n",
    "    lower = points.total_bounds[1]\n",
    "    right = points.total_bounds[2]\n",
    "    upper = points.total_bounds[3]\n",
    "    point_box = [[left, upper], [right, upper], [right, lower], [left, lower], [left, upper]]\n",
    "\n",
    "    region = ee.Geometry.Polygon(\n",
    "        [\n",
    "            point_box\n",
    "        ],\n",
    "        None,\n",
    "        False,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    image = (\n",
    "    ee.ImageCollection('USDA/NAIP/DOQQ')\n",
    "    .filterBounds(region)\n",
    "    .filterDate('2014-01-01', '2015-12-31')  ### need to figure out how to define the years\n",
    "    .mosaic()\n",
    "    .clip(region)\n",
    "    .select('R', 'G', 'B', 'N')\n",
    "    )\n",
    "    \n",
    "    params = {\n",
    "    'count': 100,  # How many image chips to export\n",
    "    'buffer': 127,  # The buffer distance (m) around each point\n",
    "    'scale': 100,  # The scale to do stratified sampling\n",
    "    'seed': 1,  # A randomization seed to use for subsampling.\n",
    "    'dimensions': '256x256',  # The dimension of each image chip\n",
    "    'format': \"GEO_TIFF\",  # The output image format, can be png, jpg, ZIPPED_GEO_TIFF, GEO_TIFF, NPY\n",
    "    'prefix': f'{optString}_chip_{DEM_name}',  # The filename prefix\n",
    "    'processes': 25,  # How many processes to used for parallel processing\n",
    "    'out_dir': f'NAIP/CHIPS_{inputName}',  # The output directory. Default to the current working directly\n",
    "    }\n",
    "\n",
    "    \n",
    "    @retry(tries=10, delay=1, backoff=2)\n",
    "    def getResult(index, point):\n",
    "        point = ee.Geometry.Point(point['coordinates'])\n",
    "        region = point.buffer(params['buffer']).bounds()\n",
    "\n",
    "        if params['format'] in ['png', 'jpg']:\n",
    "            url = image.getThumbURL(\n",
    "                {\n",
    "                    'region': region,\n",
    "                    'dimensions': params['dimensions'],\n",
    "                    'format': params['format'],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            url = image.getDownloadURL(\n",
    "                {\n",
    "                    'region': region,\n",
    "                    'dimensions': params['dimensions'],\n",
    "                    'format': params['format'],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if params['format'] == \"GEO_TIFF\":\n",
    "            ext = 'tif'\n",
    "        else:\n",
    "            ext = params['format']\n",
    "\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code != 200:\n",
    "            r.raise_for_status()\n",
    "\n",
    "        out_dir = os.path.abspath(params['out_dir'])\n",
    "        basename = f'_{round(point[\"coordinates\"][0],4)}_{round(point[\"coordinates\"][1],4)}'\n",
    "        filename = f\"{out_dir}/{params['prefix']}{basename}.{ext}\"\n",
    "        with open(filename, 'wb') as out_file:\n",
    "            shutil.copyfileobj(r.raw, out_file)\n",
    "        return out_dir\n",
    "        #print(\"Done: \", basename)\n",
    "\n",
    "        \n",
    "\n",
    "    Parallel(n_jobs=40, prefer=\"threads\")(delayed(getResult)(i, points_gee[i]) for i in tqdm(range(0, len(points_gee))))\n",
    "    #for i in tqdm(range(0, len(points_gee))):\n",
    "    #    getResult(i, points_gee[i])\n",
    "    \n",
    "    out_dir = os.path.abspath(params['out_dir'])\n",
    "    print (\"All NAIP image chips downloaded\")\n",
    "    \n",
    "    return out_dir\n",
    "\n",
    "\n",
    "\n",
    "def change_dir(DataFolder):\n",
    "    os.chdir(DataFolder)\n",
    "    dirList = ['DEM', 'NAIP', 'StreamVectors', 'POINTS', 'OUTPUT']\n",
    "    for x in dirList:\n",
    "        if not os.path.isdir(x):\n",
    "            os.mkdir(x)\n",
    "\n",
    "            \n",
    "            \n",
    "def MakeTiles(DEM_path, num):\n",
    "    #get names of folder and DEM\n",
    "    DEM_name = os.path.splitext(DEM_path)[0].split('/')[-1]\n",
    "    DEM_folder  = 'DEM'\n",
    "    \n",
    "    DEM = rasterio.open(DEM_path)\n",
    "    \n",
    "    #tiles_dir = f'{DEM_folder}/{DEM_name}_tiles'\n",
    "    \n",
    "    #if not os.path.isdir(tiles_dir):\n",
    "    #    os.mkdir(tiles_dir)\n",
    "    \n",
    "    # Set initial state\n",
    "    left_bound = DEM.bounds[0] \n",
    "    upper = DEM.bounds[3]\n",
    "    \n",
    "    # width and length, calculate new width and length\n",
    "    width = DEM.bounds[0] - DEM.bounds[2]\n",
    "    length = DEM.bounds[1] - DEM.bounds[3]\n",
    "    new_width = abs(width / num)\n",
    "    new_length = abs(length / num)\n",
    "    \n",
    "    #simple counter to keep track of the nubmer of times we've iterated.  \n",
    "    counter_x = 0\n",
    "    \n",
    "    # will increase at num**2 speed\n",
    "    name_counter = 1\n",
    "    \n",
    "    # set dictionary to capture all the bounding boxes\n",
    "    crop_polygons_bbox = {}\n",
    "    \n",
    "    while counter_x < num:\n",
    "        # will reset for each row\n",
    "        left = left_bound\n",
    "        \n",
    "        for n in range(num):\n",
    "    \n",
    "            right = left + new_width\n",
    "            lower = upper - new_length\n",
    "    \n",
    "            box = [[left, upper], [right, upper], [right, lower], [left, lower]]\n",
    "    \n",
    "            crop_polygons_bbox[name_counter] = box\n",
    "    \n",
    "            left += new_width\n",
    "    \n",
    "            name_counter += 1\n",
    "    \n",
    "        upper -= new_length\n",
    "        counter_x += 1\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    # make a list to catch each data frame\n",
    "    geoSeries_list = []\n",
    "    \n",
    "    for eachBBOX in crop_polygons_bbox:\n",
    "        \n",
    "        # define the geometry\n",
    "        part_polygon = Polygon(crop_polygons_bbox[eachBBOX])\n",
    "        d = {'name':[f'{eachBBOX}',], 'geometry': [part_polygon,]}\n",
    "        \n",
    "        # make the geodataframe\n",
    "        DataFrame = gpd.GeoDataFrame(d, crs=str(DEM.crs))\n",
    "        # put it in the list\n",
    "        geoSeries_list.append(DataFrame)\n",
    "    \n",
    "    \n",
    "    #combine all geodataframes into one\n",
    "    all_crop_polygons = pd.concat(geoSeries_list)\n",
    "    \n",
    "    clipped_path_list = []\n",
    "    \n",
    "    for eachPolygon in all_crop_polygons.geometry:\n",
    "    \n",
    "    \n",
    "        left = eachPolygon.bounds[0] \n",
    "        lower = eachPolygon.bounds[1]\n",
    "        right = eachPolygon.bounds[2]\n",
    "        upper = eachPolygon.bounds[3]\n",
    "        f_name_coord = f'left_{str(round(left, 2))}_upper_{str(round(upper, 2))}_right_{str(round(right, 2))}_lower_{str(round(lower, 2))}'\n",
    "    \n",
    "        out_tif = f'OUTPUT/{DEM_name}_{f_name_coord}.tif'\n",
    "    \n",
    "        # Set NAIP bounding box\n",
    "        point_box = [[left, upper], [right, upper], [right, lower], [left, lower]]\n",
    "    \n",
    "        geometries = [{'type': 'Polygon',\n",
    "                       'coordinates': [point_box]}]\n",
    "    \n",
    "    \n",
    "        out_img, out_transform = mask(DEM, shapes=geometries, crop=True,  all_touched=True)\n",
    "    \n",
    "    \n",
    "        # Copy the metadata\n",
    "        out_meta = DEM.meta.copy()\n",
    "        epsg_code = int(DEM.crs.data['init'][5:])\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_img.shape[1],\n",
    "                         \"width\": out_img.shape[2],\n",
    "                         \"transform\": out_transform,\n",
    "                         \"crs\": DEM.crs}\n",
    "                       ) \n",
    "    \n",
    "        clipped_path_list.append(out_tif)\n",
    "        with rasterio.open(out_tif, \"w\", **out_meta) as dest:\n",
    "    \n",
    "            dest.write(out_img)\n",
    "    return clipped_path_list\n",
    "\n",
    "\n",
    "\n",
    "def crop_byBoundary(raster_path, cropBoundary_SHP):\n",
    "    \n",
    "    raster = rasterio.open(raster_path)\n",
    "    raster_name = os.path.split(raster_path)[-1].split('.')[0]\n",
    "\n",
    "    try:        \n",
    "        crop_poly = gpd.read_file(cropBoundary_SHP)\n",
    "        crop_project = crop_poly.to_crs(\"EPSG:3857\")\n",
    "\n",
    "        crop_buff = crop_project.buffer(1000, cap_style=3)\n",
    "        crop_repro = crop_buff.to_crs(raster.crs)\n",
    "        \n",
    "\n",
    "        out_tif = os.path.join((os.path.split(raster_path)[0]),f'{raster_name}_cropped.tif')\n",
    "\n",
    "        left = crop_repro.total_bounds[0] \n",
    "        lower = crop_repro.total_bounds[1]\n",
    "        right = crop_repro.total_bounds[2]\n",
    "        upper = crop_repro.total_bounds[3]\n",
    "\n",
    "        # Set raster bounding box\n",
    "        raster_box = [[left, upper], [right, upper], [right, lower], [left, lower], [left, upper]]\n",
    "\n",
    "        geometries = [{'type': 'Polygon',\n",
    "                           'coordinates': [raster_box]}]\n",
    "\n",
    "\n",
    "        out_img, out_transform = mask(raster, shapes=geometries, crop=True,  all_touched=True)\n",
    "\n",
    "\n",
    "        # Copy the metadata\n",
    "        out_meta = raster.meta.copy()\n",
    "        epsg_code = int(raster.crs.data['init'][5:])\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_img.shape[1],\n",
    "                         \"width\": out_img.shape[2],\n",
    "                         \"transform\": out_transform,\n",
    "                         \"crs\": raster.crs}\n",
    "                       ) \n",
    "\n",
    "\n",
    "        with rasterio.open(out_tif, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_img)\n",
    "\n",
    "        return out_tif\n",
    "    except: \n",
    "        return\n",
    "    \n",
    "\n",
    "\n",
    "def deleteOldFiles(folder):\n",
    "    dirFolder = os.path.join((os.getcwd()),folder)\n",
    "\n",
    "\n",
    "    root_name = []\n",
    "    for eachFile in glob.glob(dirFolder + '/*'):\n",
    "        a = '_'.join(os.path.split(eachFile)[-1].split('.')[0].split('_')[:3])\n",
    "        if a not in root_name:\n",
    "            root_name.append(a)\n",
    "\n",
    "\n",
    "    newestFile = []\n",
    "    for name in root_name:\n",
    "        x = max(glob.glob(os.path.join(dirFolder,name+'*')))\n",
    "        newestFile.append(x)\n",
    "\n",
    "    for file in glob.glob(os.path.join(dirFolder,'*')):\n",
    "        if file not in newestFile:\n",
    "            os.remove(file)\n",
    "    \n",
    "    return glob.glob(os.path.join(dirFolder,'*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5407c28-1585-442d-aa80-49978472909e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Location and folder structure - local machine for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5edd4102-872f-4b91-b5a6-ad17f12f4aef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input (existing) folder path for Data folder structure: /Volumes/Lightning Strike/EARTHSHOT_DATA/_BEAVER_DAM_DATES_2014_2015\n"
     ]
    }
   ],
   "source": [
    "DataFolder = input('Input (existing) folder path for Data folder structure:')  \n",
    "#DataFolder = ''\n",
    "change_dir(DataFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb8482-dce0-4c79-ab64-ca31a2b66c90",
   "metadata": {
    "tags": []
   },
   "source": [
    "### File input:\n",
    "#### Currently accepts .shp, .tif files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38b830-4070-40db-81ad-62a3f3400dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#InputData = input('Input file path:')   # example: _SAMPLE/03_OregonDams_shp.shp\n",
    "\n",
    "InputData = '_SAMPLE_DAM_SHAPEFILES/03_OregonDams_shp.shp'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5356bc07-8cde-4a37-99c2-41b848ba1516",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run it all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5677d4-2c8f-4c31-be62-81e7829f08a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "### This bit of code hasn't been updated in a while. But this is the intended section to run on a single AOI\n",
    "\n",
    "%%time\n",
    "warnings.filterwarnings(\"ignore\")  ## optional. I'm gettign an number of depreciated warnings... \n",
    "Download_DEM(InputData)\n",
    "inputName = os.path.split(InputData)[-1].split('.')[0]\n",
    "\n",
    "print (\"Download Complete\")\n",
    "\n",
    "deleteOldFiles('DEM')\n",
    "\n",
    "for eachDEM in glob.glob(os.path.join((os.getcwd()),f'DEM/{inputName}/*')):\n",
    "    \n",
    "    DEM_path = crop_byBoundary(eachDEM, InputData)\n",
    "    \n",
    "    DEM_name = os.path.split(eachDEM)[-1].split('.')[0]\n",
    "    DEM = rasterio.open(DEM_path)\n",
    "    \n",
    "    dirList = [f'NAIP/CHIPS_{DEM_name}']\n",
    "    for x in dirList:\n",
    "        if not os.path.isdir(x):\n",
    "            os.mkdir(x)\n",
    "    \n",
    "    streamNet = watershed_preProcessing(DEM_path)\n",
    "    print (\"Watershed pre-Processing Complete\")\n",
    "    \n",
    "    points, points_path = makePointsFromLines(streamNet, 100)\n",
    "    print (\"Points Creation Complete\")\n",
    "    \n",
    "    points = gpd.read_file(points_path)\n",
    "    \n",
    "    print (\"Now, creating NAIP chips. This may take a while depending on the size of area.\")\n",
    "    NAIP_Chips_folder = points_toNAIP_Chips(points, inputName)\n",
    "\n",
    "print (\"All done!\")\n",
    "warnings.resetwarnings()  ## optional. I'm gettign an number of depreciated warnings... \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c3cb4e-49ed-4abf-b41b-9722aba25320",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Looped running for each dam site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e816a1c-4ebb-4e87-8604-bcfae0f42f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on location 1 of 6...\n",
      "Downloading USGS_13_n41w107_20220216.tif file 1 of 3\n",
      "Downloading USGS_13_n41w107_20130911.tif file 2 of 3\n",
      "Downloading USGS_13_n42w107_20180313.tif file 3 of 3\n",
      "Download Complete\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "read_raster() missing 1 required positional argument: 'data_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/var/folders/k1/bflx0n7s7qjgp5981_hcy8t80000gn/T/ipykernel_62714/1635272307.py\u001b[0m in \u001b[0;36mwatershed_preProcessing\u001b[0;34m(dem_path)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_raster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdem_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dem'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mdem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_raster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdem_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# Detect pits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: read_raster() missing 1 required positional argument: 'data_name'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_counter = 1\n",
    "\n",
    "## Input your own new file path - outside of the repo. This downloads a LOT of files.  \n",
    "for InputData in glob.glob('/Users/forrestpound/Dropbox (Personal)/_PROJECTS/ACTIVE/EarthShot/Earthshot_riparian_image_creation/_SAMPLE_DAM_SHAPEFILES/*.shp'):\n",
    "    print(f\"Working on location {n_counter} of {len(glob.glob('/Users/forrestpound/Dropbox (Personal)/_PROJECTS/ACTIVE/EarthShot/Earthshot_riparian_image_creation/_SAMPLE_DAM_SHAPEFILES/*.shp'))}...\")\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")  ## optional. I'm getting an number of depreciated warnings... \n",
    "    rasterList = Download_DEM(InputData)\n",
    "    \n",
    "    print (\"Download Complete\")\n",
    "\n",
    "    rasterList = deleteOldFiles(os.path.dirname(rasterList[0]))  \n",
    "    \n",
    "    \n",
    "    for eachDEM in rasterList:\n",
    "\n",
    "        \n",
    "        DEM_path = crop_byBoundary(eachDEM, InputData)\n",
    "        \n",
    "        if DEM_path is not None:\n",
    "            \n",
    "        \n",
    "            DEM_name = os.path.split(eachDEM)[-1].split('.')[0]\n",
    "            DEM = rasterio.open(DEM_path)\n",
    "\n",
    "            inputName = os.path.split(InputData)[-1].split('.')[0]\n",
    "\n",
    "            dirList = [f'NAIP/CHIPS_{inputName}']\n",
    "            for x in dirList:\n",
    "                if not os.path.isdir(x):\n",
    "                    os.mkdir(x)\n",
    "\n",
    "            streamNet = watershed_preProcessing(DEM_path)\n",
    "            print (\"Watershed pre-Processing Complete\")\n",
    "\n",
    "\n",
    "            points, points_path = makePointsFromLines(streamNet, 100)\n",
    "            print (\"Points Creation Complete\")\n",
    "\n",
    "            ## Set up the yes/no dam question for training data\n",
    "            buff = gpd.read_file(points_path).buffer(128, cap_style=3)\n",
    "            buff.to_file(f'POINTS/{inputName}_tempOUT.shp')\n",
    "            buff_gdf = gpd.read_file(f'POINTS/{inputName}_tempOUT.shp')\n",
    "\n",
    "            dams = gpd.read_file(InputData)\n",
    "            damsRepro = dams.to_crs(buff.crs)\n",
    "            dams_union = damsRepro.geometry.unary_union\n",
    "\n",
    "\n",
    "            print (f\"Now, creating NAIP chips for {inputName}, yes beaver dam intersect.\")\n",
    "\n",
    "            ## Yes, there is an intersection between image chip and beaver dam, output images\n",
    "            yes_dams = buff_gdf[buff_gdf.geometry.intersects(dams_union)]\n",
    "            yes_path = f'POINTS/{inputName}_yes_dams.shp'\n",
    "            yes_dams.to_file(yes_path)\n",
    "            yes_gdf = gpd.read_file(yes_path)\n",
    "            yes_repro = yes_gdf.to_crs(DEM.crs)\n",
    "            NAIP_Chips_folder = points_toNAIP_Chips(yes_repro, inputName, 'yes')\n",
    "\n",
    "            print (f\"Now, creating NAIP chips for {inputName}, no beaver dam intersect.\")\n",
    "            ## No, there is an intersection between image chip and beaver dam, output images\n",
    "            no_dams = buff_gdf[~buff_gdf.geometry.intersects(dams_union)]\n",
    "            no_path = f'POINTS/{inputName}_no_dams.shp'\n",
    "            no_dams.to_file(no_path)\n",
    "            no_gdf = gpd.read_file(no_path)\n",
    "            no_repro = no_gdf.to_crs(DEM.crs)\n",
    "            NAIP_Chips_folder = points_toNAIP_Chips(no_repro, inputName,'no')\n",
    "\n",
    "          \n",
    "    n_counter += 1\n",
    "print (\"All done!\")\n",
    "warnings.resetwarnings()  ## optional. I'm getting an number of depreciated warnings... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "950545f1-a56b-48b4-a036-5d2ea5eebfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_raster in module pysheds.pgrid:\n",
      "\n",
      "from_raster(path, data_name, **kwargs) method of builtins.type instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Grid.from_raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57e1907b-9877-414f-a376-e1764cc829fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_raster in module pysheds.pgrid:\n",
      "\n",
      "read_raster(self, data, data_name, band=1, window=None, window_crs=None, metadata={}, mask_geometry=False, **kwargs)\n",
      "    Reads data from a raster file into a named attribute of Grid\n",
      "    (name of attribute determined by keyword 'data_name').\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : str\n",
      "           File name or path.\n",
      "    data_name : str\n",
      "                Name of dataset. Will determine the name of the attribute\n",
      "                representing the gridded data.\n",
      "    band : int\n",
      "           The band number to read if multiband.\n",
      "    window : tuple\n",
      "             If using windowed reading, specify window (xmin, ymin, xmax, ymax).\n",
      "    window_crs : pyproj.Proj instance\n",
      "                 Coordinate reference system of window. If None, assume it's in raster's crs.\n",
      "    mask_geometry : iterable object\n",
      "                    The values must be a GeoJSON-like dict or an object that implements\n",
      "                    the Python geo interface protocol (such as a Shapely Polygon).\n",
      "    metadata : dict\n",
      "               Other attributes describing dataset, such as direction\n",
      "               mapping for flow direction files. e.g.:\n",
      "               metadata={'dirmap' : (64, 128, 1, 2, 4, 8, 16, 32),\n",
      "                         'routing' : 'd8'}\n",
      "    \n",
      "    Additional keyword arguments are passed to rasterio.open()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Grid.read_raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c3a3e-af78-473a-87a8-eba1a01e6cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
